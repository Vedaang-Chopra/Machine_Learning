{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#\n",
    "# Cache-wrapper for a function or class.\n",
    "#\n",
    "# Save the result of calling a function or creating an object-instance\n",
    "# to harddisk. This is used to persist the data so it can be reloaded\n",
    "# very quickly and easily.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "########################################################################\n",
    "#\n",
    "# This file is part of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def cache(cache_path, fn, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Cache-wrapper for a function or class. If the cache-file exists\n",
    "    then the data is reloaded and returned, otherwise the function\n",
    "    is called and the result is saved to cache. The fn-argument can\n",
    "    also be a class instead, in which case an object-instance is\n",
    "    created and saved to the cache-file.\n",
    "\n",
    "    :param cache_path:\n",
    "        File-path for the cache-file.\n",
    "\n",
    "    :param fn:\n",
    "        Function or class to be called.\n",
    "\n",
    "    :param args:\n",
    "        Arguments to the function or class-init.\n",
    "\n",
    "    :param kwargs:\n",
    "        Keyword arguments to the function or class-init.\n",
    "\n",
    "    :return:\n",
    "        The result of calling the function or creating the object-instance.\n",
    "    \"\"\"\n",
    "\n",
    "    # If the cache-file exists.\n",
    "    if os.path.exists(cache_path):\n",
    "        # Load the cached data from the file.\n",
    "        with open(cache_path, mode='rb') as file:\n",
    "            obj = pickle.load(file)\n",
    "\n",
    "        print(\"- Data loaded from cache-file: \" + cache_path)\n",
    "    else:\n",
    "        # The cache-file does not exist.\n",
    "\n",
    "        # Call the function / class-init with the supplied arguments.\n",
    "        obj = fn(*args, **kwargs)\n",
    "\n",
    "        # Save the data to a cache-file.\n",
    "        with open(cache_path, mode='wb') as file:\n",
    "            pickle.dump(obj, file)\n",
    "\n",
    "        print(\"- Data saved to cache-file: \" + cache_path)\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def convert_numpy2pickle(in_path, out_path):\n",
    "    \"\"\"\n",
    "    Convert a numpy-file to pickle-file.\n",
    "\n",
    "    The first version of the cache-function used numpy for saving the data.\n",
    "    Instead of re-calculating all the data, you can just convert the\n",
    "    cache-file using this function.\n",
    "\n",
    "    :param in_path:\n",
    "        Input file in numpy-format written using numpy.save().\n",
    "\n",
    "    :param out_path:\n",
    "        Output file written as a pickle-file.\n",
    "\n",
    "    :return:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data using numpy.\n",
    "    data = np.load(in_path)\n",
    "\n",
    "    # Save the data using pickle.\n",
    "    with open(out_path, mode='wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # This is a short example of using a cache-file.\n",
    "\n",
    "    # This is the function that will only get called if the result\n",
    "    # is not already saved in the cache-file. This would normally\n",
    "    # be a function that takes a long time to compute, or if you\n",
    "    # need persistent data for some other reason.\n",
    "    def expensive_function(a, b):\n",
    "        return a * b\n",
    "\n",
    "    print('Computing expensive_function() ...')\n",
    "\n",
    "    # Either load the result from a cache-file if it already exists,\n",
    "    # otherwise calculate expensive_function(a=123, b=456) and\n",
    "    # save the result to the cache-file for next time.\n",
    "    result = cache(cache_path='cache_expensive_function.pkl',\n",
    "                   fn=expensive_function, a=123, b=456)\n",
    "\n",
    "    print('result =', result)\n",
    "\n",
    "    # Newline.\n",
    "    print()\n",
    "\n",
    "    # This is another example which saves an object to a cache-file.\n",
    "\n",
    "    # We want to cache an object-instance of this class.\n",
    "    # The motivation is to do an expensive computation only once,\n",
    "    # or if we need to persist the data for some other reason.\n",
    "    class ExpensiveClass:\n",
    "        def __init__(self, c, d):\n",
    "            self.c = c\n",
    "            self.d = d\n",
    "            self.result = c * d\n",
    "\n",
    "        def print_result(self):\n",
    "            print('c =', self.c)\n",
    "            print('d =', self.d)\n",
    "            print('result = c * d =', self.result)\n",
    "\n",
    "    print('Creating object from ExpensiveClass() ...')\n",
    "\n",
    "    # Either load the object from a cache-file if it already exists,\n",
    "    # otherwise make an object-instance ExpensiveClass(c=123, d=456)\n",
    "    # and save the object to the cache-file for the next time.\n",
    "    obj = cache(cache_path='cache_ExpensiveClass.pkl',\n",
    "                fn=ExpensiveClass, c=123, d=456)\n",
    "\n",
    "    obj.print_result()\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "#\n",
    "# Functions for downloading and extracting data-files from the internet.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "########################################################################\n",
    "#\n",
    "# This file is part of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def _print_download_progress(count, block_size, total_size):\n",
    "    \"\"\"\n",
    "    Function used for printing the download progress.\n",
    "    Used as a call-back function in maybe_download_and_extract().\n",
    "    \"\"\"\n",
    "\n",
    "    # Percentage completion.\n",
    "    pct_complete = float(count * block_size) / total_size\n",
    "\n",
    "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
    "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
    "\n",
    "    # Print it.\n",
    "    sys.stdout.write(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def maybe_download_and_extract_of_download(url, download_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the data if it doesn't already exist.\n",
    "    Assumes the url is a tar-ball file.\n",
    "\n",
    "    :param url:\n",
    "        Internet URL for the tar-file to download.\n",
    "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "    :param download_dir:\n",
    "        Directory where the downloaded file is saved.\n",
    "        Example: \"data/CIFAR-10/\"\n",
    "\n",
    "    :return:\n",
    "        Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filename for saving the file downloaded from the internet.\n",
    "    # Use the filename from the URL and add it to the download_dir.\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    # Check if the file already exists.\n",
    "    # If it exists then we assume it has also been extracted,\n",
    "    # otherwise we need to download and extract it now.\n",
    "    if not os.path.exists(file_path):\n",
    "        # Check if the download directory exists, otherwise create it.\n",
    "        if not os.path.exists(download_dir):\n",
    "            os.makedirs(download_dir)\n",
    "\n",
    "        # Download the file from the internet.\n",
    "        file_path, _ = urllib.request.urlretrieve(url=url,\n",
    "                                                  filename=file_path,\n",
    "                                                  reporthook=_print_download_progress)\n",
    "\n",
    "        print()\n",
    "        print(\"Download finished. Extracting files.\")\n",
    "\n",
    "        if file_path.endswith(\".zip\"):\n",
    "            # Unpack the zip-file.\n",
    "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
    "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
    "            # Unpack the tar-ball.\n",
    "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
    "\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"Data has apparently already been downloaded and unpacked.\")\n",
    "\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "#\n",
    "# Class for creating a data-set consisting of all files in a directory.\n",
    "#\n",
    "# Example usage is shown in the file knifey.py and Tutorial #09.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "########################################################################\n",
    "#\n",
    "# This file is part of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def one_hot_encoded(class_numbers, num_classes=None):\n",
    "    \"\"\"\n",
    "    Generate the One-Hot encoded class-labels from an array of integers.\n",
    "\n",
    "    For example, if class_number=2 and num_classes=4 then\n",
    "    the one-hot encoded label is the float array: [0. 0. 1. 0.]\n",
    "\n",
    "    :param class_numbers:\n",
    "        Array of integers with class-numbers.\n",
    "        Assume the integers are from zero to num_classes-1 inclusive.\n",
    "\n",
    "    :param num_classes:\n",
    "        Number of classes. If None then use max(class_numbers)+1.\n",
    "\n",
    "    :return:\n",
    "        2-dim array of shape: [len(class_numbers), num_classes]\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the number of classes if None is provided.\n",
    "    # Assumes the lowest class-number is zero.\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(class_numbers) + 1\n",
    "\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self, in_dir, exts='.jpg'):\n",
    "        \"\"\"\n",
    "        Create a data-set consisting of the filenames in the given directory\n",
    "        and sub-dirs that match the given filename-extensions.\n",
    "\n",
    "        For example, the knifey-spoony data-set (see knifey.py) has the\n",
    "        following dir-structure:\n",
    "\n",
    "        knifey-spoony/forky/\n",
    "        knifey-spoony/knifey/\n",
    "        knifey-spoony/spoony/\n",
    "        knifey-spoony/forky/test/\n",
    "        knifey-spoony/knifey/test/\n",
    "        knifey-spoony/spoony/test/\n",
    "\n",
    "        This means there are 3 classes called: forky, knifey, and spoony.\n",
    "\n",
    "        If we set in_dir = \"knifey-spoony/\" and create a new DataSet-object\n",
    "        then it will scan through these directories and create a training-set\n",
    "        and test-set for each of these classes.\n",
    "\n",
    "        The training-set will contain a list of all the *.jpg filenames\n",
    "        in the following directories:\n",
    "\n",
    "        knifey-spoony/forky/\n",
    "        knifey-spoony/knifey/\n",
    "        knifey-spoony/spoony/\n",
    "\n",
    "        The test-set will contain a list of all the *.jpg filenames\n",
    "        in the following directories:\n",
    "\n",
    "        knifey-spoony/forky/test/\n",
    "        knifey-spoony/knifey/test/\n",
    "        knifey-spoony/spoony/test/\n",
    "\n",
    "        See the TensorFlow Tutorial #09 for a usage example.\n",
    "\n",
    "        :param in_dir:\n",
    "            Root-dir for the files in the data-set.\n",
    "            This would be 'knifey-spoony/' in the example above.\n",
    "\n",
    "        :param exts:\n",
    "            String or tuple of strings with valid filename-extensions.\n",
    "            Not case-sensitive.\n",
    "\n",
    "        :return:\n",
    "            Object instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extend the input directory to the full path.\n",
    "        in_dir = os.path.abspath(in_dir)\n",
    "\n",
    "        # Input directory.\n",
    "        self.in_dir = in_dir\n",
    "\n",
    "        # Convert all file-extensions to lower-case.\n",
    "        self.exts = tuple(ext.lower() for ext in exts)\n",
    "\n",
    "        # Names for the classes.\n",
    "        self.class_names = []\n",
    "\n",
    "        # Filenames for all the files in the training-set.\n",
    "        self.filenames = []\n",
    "\n",
    "        # Filenames for all the files in the test-set.\n",
    "        self.filenames_test = []\n",
    "\n",
    "        # Class-number for each file in the training-set.\n",
    "        self.class_numbers = []\n",
    "\n",
    "        # Class-number for each file in the test-set.\n",
    "        self.class_numbers_test = []\n",
    "\n",
    "        # Total number of classes in the data-set.\n",
    "        self.num_classes = 0\n",
    "\n",
    "        # For all files/dirs in the input directory.\n",
    "        for name in os.listdir(in_dir):\n",
    "            # Full path for the file / dir.\n",
    "            current_dir = os.path.join(in_dir, name)\n",
    "\n",
    "            # If it is a directory.\n",
    "            if os.path.isdir(current_dir):\n",
    "                # Add the dir-name to the list of class-names.\n",
    "                self.class_names.append(name)\n",
    "\n",
    "                # Training-set.\n",
    "\n",
    "                # Get all the valid filenames in the dir (not sub-dirs).\n",
    "                filenames = self._get_filenames(current_dir)\n",
    "\n",
    "                # Append them to the list of all filenames for the training-set.\n",
    "                self.filenames.extend(filenames)\n",
    "\n",
    "                # The class-number for this class.\n",
    "                class_number = self.num_classes\n",
    "\n",
    "                # Create an array of class-numbers.\n",
    "                class_numbers = [class_number] * len(filenames)\n",
    "\n",
    "                # Append them to the list of all class-numbers for the training-set.\n",
    "                self.class_numbers.extend(class_numbers)\n",
    "\n",
    "                # Test-set.\n",
    "\n",
    "                # Get all the valid filenames in the sub-dir named 'test'.\n",
    "                filenames_test = self._get_filenames(os.path.join(current_dir, 'test'))\n",
    "\n",
    "                # Append them to the list of all filenames for the test-set.\n",
    "                self.filenames_test.extend(filenames_test)\n",
    "\n",
    "                # Create an array of class-numbers.\n",
    "                class_numbers = [class_number] * len(filenames_test)\n",
    "\n",
    "                # Append them to the list of all class-numbers for the test-set.\n",
    "                self.class_numbers_test.extend(class_numbers)\n",
    "\n",
    "                # Increase the total number of classes in the data-set.\n",
    "                self.num_classes += 1\n",
    "\n",
    "    def _get_filenames(self, dir):\n",
    "        \"\"\"\n",
    "        Create and return a list of filenames with matching extensions in the given directory.\n",
    "\n",
    "        :param dir:\n",
    "            Directory to scan for files. Sub-dirs are not scanned.\n",
    "\n",
    "        :return:\n",
    "            List of filenames. Only filenames. Does not include the directory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize empty list.\n",
    "        filenames = []\n",
    "\n",
    "        # If the directory exists.\n",
    "        if os.path.exists(dir):\n",
    "            # Get all the filenames with matching extensions.\n",
    "            for filename in os.listdir(dir):\n",
    "                if filename.lower().endswith(self.exts):\n",
    "                    filenames.append(filename)\n",
    "\n",
    "        return filenames\n",
    "\n",
    "    def get_paths(self, test=False):\n",
    "        \"\"\"\n",
    "        Get the full paths for the files in the data-set.\n",
    "\n",
    "        :param test:\n",
    "            Boolean. Return the paths for the test-set (True) or training-set (False).\n",
    "\n",
    "        :return:\n",
    "            Iterator with strings for the path-names.\n",
    "        \"\"\"\n",
    "\n",
    "        if test:\n",
    "            # Use the filenames and class-numbers for the test-set.\n",
    "            filenames = self.filenames_test\n",
    "            class_numbers = self.class_numbers_test\n",
    "\n",
    "            # Sub-dir for test-set.\n",
    "            test_dir = \"test/\"\n",
    "        else:\n",
    "            # Use the filenames and class-numbers for the training-set.\n",
    "            filenames = self.filenames\n",
    "            class_numbers = self.class_numbers\n",
    "\n",
    "            # Don't use a sub-dir for test-set.\n",
    "            test_dir = \"\"\n",
    "\n",
    "        for filename, cls in zip(filenames, class_numbers):\n",
    "            # Full path-name for the file.\n",
    "            path = os.path.join(self.in_dir, self.class_names[cls], test_dir, filename)\n",
    "\n",
    "            yield path\n",
    "\n",
    "    def get_training_set(self):\n",
    "        \"\"\"\n",
    "        Return the list of paths for the files in the training-set,\n",
    "        and the list of class-numbers as integers,\n",
    "        and the class-numbers as one-hot encoded arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        return list(self.get_paths()), \\\n",
    "               np.asarray(self.class_numbers), \\\n",
    "               one_hot_encoded(class_numbers=self.class_numbers,\n",
    "                               num_classes=self.num_classes)\n",
    "\n",
    "    def get_test_set(self):\n",
    "        \"\"\"\n",
    "        Return the list of paths for the files in the test-set,\n",
    "        and the list of class-numbers as integers,\n",
    "        and the class-numbers as one-hot encoded arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        return list(self.get_paths(test=True)), \\\n",
    "               np.asarray(self.class_numbers_test), \\\n",
    "               one_hot_encoded(class_numbers=self.class_numbers_test,\n",
    "                               num_classes=self.num_classes)\n",
    "\n",
    "\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def load_cached(cache_path, in_dir):\n",
    "    \"\"\"\n",
    "    Wrapper-function for creating a DataSet-object, which will be\n",
    "    loaded from a cache-file if it already exists, otherwise a new\n",
    "    object will be created and saved to the cache-file.\n",
    "\n",
    "    This is useful if you need to ensure the ordering of the\n",
    "    filenames is consistent every time you load the data-set,\n",
    "    for example if you use the DataSet-object in combination\n",
    "    with Transfer Values saved to another cache-file, see e.g.\n",
    "    Tutorial #09 for an example of this.\n",
    "\n",
    "    :param cache_path:\n",
    "        File-path for the cache-file.\n",
    "\n",
    "    :param in_dir:\n",
    "        Root-dir for the files in the data-set.\n",
    "        This is an argument for the DataSet-init function.\n",
    "\n",
    "    :return:\n",
    "        The DataSet-object.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Creating dataset from the files in: \" + in_dir)\n",
    "\n",
    "    # If the object-instance for DataSet(in_dir=data_dir) already\n",
    "    # exists in the cache-file then reload it, otherwise create\n",
    "    # an object instance and save it to the cache-file for next time.\n",
    "    dataset = cache(cache_path=cache_path,\n",
    "                    fn=DataSet, in_dir=in_dir)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "#\n",
    "# Functions for downloading the CIFAR-10 data-set from the internet\n",
    "# and loading it into memory.\n",
    "#\n",
    "# Implemented in Python 3.5\n",
    "#\n",
    "# Usage:\n",
    "# 1) Set the variable data_path with the desired storage path.\n",
    "# 2) Call maybe_download_and_extract() to download the data-set\n",
    "#    if it is not already located in the given data_path.\n",
    "# 3) Call load_class_names() to get an array of the class-names.\n",
    "# 4) Call load_training_data() and load_test_data() to get\n",
    "#    the images, class-numbers and one-hot encoded class-labels\n",
    "#    for the training-set and test-set.\n",
    "# 5) Use the returned data in your own program.\n",
    "#\n",
    "# Format:\n",
    "# The images for the training- and test-sets are returned as 4-dim numpy\n",
    "# arrays each with the shape: [image_number, height, width, channel]\n",
    "# where the individual pixels are floats between 0.0 and 1.0.\n",
    "#\n",
    "########################################################################\n",
    "#\n",
    "# This file is part of the TensorFlow Tutorials available at:\n",
    "#\n",
    "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
    "#\n",
    "# Published under the MIT License. See the file LICENSE for details.\n",
    "#\n",
    "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
    "#\n",
    "########################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# Directory where you want to download and save the data-set.\n",
    "# Set this before you start calling any of the functions below.\n",
    "data_path = \"data/CIFAR-10/\"\n",
    "\n",
    "# URL for the data-set on the internet.\n",
    "data_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "########################################################################\n",
    "# Various constants for the size of the images.\n",
    "# Use these constants in your own program.\n",
    "\n",
    "# Width and height of each image.\n",
    "img_size = 32\n",
    "\n",
    "# Number of channels in each image, 3 channels: Red, Green, Blue.\n",
    "num_channels = 3\n",
    "\n",
    "# Length of an image when flattened to a 1-dim array.\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "\n",
    "# Number of classes.\n",
    "num_classes = 10\n",
    "\n",
    "########################################################################\n",
    "# Various constants used to allocate arrays of the correct size.\n",
    "\n",
    "# Number of files for the training-set.\n",
    "_num_files_train = 5\n",
    "\n",
    "# Number of images for each batch-file in the training-set.\n",
    "_images_per_file = 10000\n",
    "\n",
    "# Total number of images in the training-set.\n",
    "# This is used to pre-allocate arrays for efficiency.\n",
    "_num_images_train = _num_files_train * _images_per_file\n",
    "\n",
    "########################################################################\n",
    "# Private functions for downloading, unpacking and loading data-files.\n",
    "\n",
    "\n",
    "def _get_file_path(filename=\"\"):\n",
    "    \"\"\"\n",
    "    Return the full path of a data-file for the data-set.\n",
    "\n",
    "    If filename==\"\" then return the directory of the files.\n",
    "    \"\"\"\n",
    "\n",
    "    return os.path.join(data_path, \"cifar-10-batches-py/\", filename)\n",
    "\n",
    "\n",
    "def _unpickle(filename):\n",
    "    \"\"\"\n",
    "    Unpickle the given file and return the data.\n",
    "\n",
    "    Note that the appropriate dir-name is prepended the filename.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create full path for the file.\n",
    "    file_path = _get_file_path(filename)\n",
    "\n",
    "    print(\"Loading data: \" + file_path)\n",
    "\n",
    "    with open(file_path, mode='rb') as file:\n",
    "        # In Python 3.X it is important to set the encoding,\n",
    "        # otherwise an exception is raised here.\n",
    "        data = pickle.load(file,  encoding='latin1')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _convert_images(raw):\n",
    "    \"\"\"\n",
    "    Convert images from the CIFAR-10 format and\n",
    "    return a 4-dim array with shape: [image_number, height, width, channel]\n",
    "    where the pixels are floats between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the raw images from the data-files to floating-points.\n",
    "    raw_float = np.array(raw, dtype=float) / 255.0\n",
    "\n",
    "    # Reshape the array to 4-dimensions.\n",
    "    images = raw_float.reshape([-1, num_channels, img_size, img_size])\n",
    "\n",
    "    # Reorder the indices of the array.\n",
    "    images = images.transpose([0, 2, 3, 1])\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def _load_data(filename):\n",
    "    \"\"\"\n",
    "    Load a pickled data-file from the CIFAR-10 data-set\n",
    "    and return the converted images (see above) and the class-number\n",
    "    for each image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pickled data-file.\n",
    "    data = _unpickle(filename)\n",
    "\n",
    "    # Get the raw images.\n",
    "    raw_images = data['data']\n",
    "\n",
    "    # Get the class-numbers for each image. Convert to numpy-array.\n",
    "    cls = np.array(data['labels'])\n",
    "\n",
    "    # Convert the images.\n",
    "    images = _convert_images(raw_images)\n",
    "\n",
    "    return images, cls\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Public functions that you may call to download the data-set from\n",
    "# the internet and load the data into memory.\n",
    "\n",
    "\n",
    "def maybe_download_and_extract():\n",
    "    \"\"\"\n",
    "    Download and extract the CIFAR-10 data-set if it doesn't already exist\n",
    "    in data_path (set this variable first to the desired path).\n",
    "    \"\"\"\n",
    "\n",
    "    maybe_download_and_extract_of_download(url=data_url, download_dir=data_path)\n",
    "\n",
    "\n",
    "def load_class_names():\n",
    "    \"\"\"\n",
    "    Load the names for the classes in the CIFAR-10 data-set.\n",
    "\n",
    "    Returns a list with the names. Example: names[3] is the name\n",
    "    associated with class-number 3.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the class-names from the pickled file.\n",
    "    raw = _unpickle(filename=\"batches.meta\")['label_names']\n",
    "\n",
    "    # Convert from binary strings.\n",
    "    return raw\n",
    "\n",
    "\n",
    "def load_training_data():\n",
    "    \"\"\"\n",
    "    Load all the training-data for the CIFAR-10 data-set.\n",
    "\n",
    "    The data-set is split into 5 data-files which are merged here.\n",
    "\n",
    "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pre-allocate the arrays for the images and class-numbers for efficiency.\n",
    "    images = np.zeros(shape=[_num_images_train, img_size, img_size, num_channels], dtype=float)\n",
    "    cls = np.zeros(shape=[_num_images_train], dtype=int)\n",
    "\n",
    "    # Begin-index for the current batch.\n",
    "    begin = 0\n",
    "\n",
    "    # For each data-file.\n",
    "    for i in range(_num_files_train):\n",
    "        # Load the images and class-numbers from the data-file.\n",
    "        images_batch, cls_batch = _load_data(filename=\"data_batch_\" + str(i + 1))\n",
    "\n",
    "        # Number of images in this batch.\n",
    "        num_images = len(images_batch)\n",
    "\n",
    "        # End-index for the current batch.\n",
    "        end = begin + num_images\n",
    "\n",
    "        # Store the images into the array.\n",
    "        images[begin:end, :] = images_batch\n",
    "\n",
    "        # Store the class-numbers into the array.\n",
    "        cls[begin:end] = cls_batch\n",
    "\n",
    "        # The begin-index for the next batch is the current end-index.\n",
    "        begin = end\n",
    "\n",
    "    return images, cls, one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    Load all the test-data for the CIFAR-10 data-set.\n",
    "\n",
    "    Returns the images, class-numbers and one-hot encoded class-labels.\n",
    "    \"\"\"\n",
    "\n",
    "    images, cls = _load_data(filename=\"test_batch\")\n",
    "\n",
    "    return images, cls, one_hot_encoded(class_numbers=cls, num_classes=num_classes)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "maybe_download_and_extract()\n",
    "class_names = load_class_names()\n",
    "images_train, cls_train, labels_train = load_training_data()\n",
    "images_test, cls_test, labels_test = load_test_data()\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from google.colab import files\n",
    "import csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(type(images_train), type(cls_train), type(labels_train))\n",
    "print(images_train.shape, cls_train.shape, labels_train.shape)\n",
    "# print(images_train[0][0])\n",
    "# for i in range (len(labels_train)):\n",
    "#     print(labels_train[i])\n",
    "def PCA_implementation(images_train, images_test):\n",
    "    # fig = plt.figure(figsize=(10, 10))\n",
    "    # for i in range(100):\n",
    "    #     ax = fig.add_subplot(10, 10, i + 1)\n",
    "    #     ax.imshow(images_train[i], cmap=plt.cm.bone)\n",
    "    # plt.show()\n",
    "    x = images_train.copy()\n",
    "    x_test = images_test.copy()\n",
    "    x = x.reshape((50000, 32 * 32 * 3))\n",
    "    x_test = x_test.reshape((10000, 32 * 32 * 3))\n",
    "    x = preprocessing.scale(x)\n",
    "    x_test = preprocessing.scale(x_test)\n",
    "    pca1 = PCA()\n",
    "    pca1.fit(x)\n",
    "    k = 0\n",
    "    current_sum = 0\n",
    "    total = sum(pca1.explained_variance_)\n",
    "    while current_sum / total < 0.99:\n",
    "        current_sum += pca1.explained_variance_[k]\n",
    "        k = k + 1\n",
    "    pca = PCA(n_components=k, whiten=True)\n",
    "    x_transform_pca = pca.fit_transform(x)\n",
    "    x_test_pca=pca.transform(x_test)\n",
    "    x_approx = pca.inverse_transform(x_transform_pca)\n",
    "#     x_approx.reshape((50000, 32, 32, 3))\n",
    "#     fig = plt.figure(figsize=(10, 10))\n",
    "#     for i in range(100):\n",
    "#         ax = fig.add_subplot(10, 10, i + 1)\n",
    "#         ax.imshow(x_approx[i], cmap=plt.cm.bone)\n",
    "#     plt.show()\n",
    "    return x_transform_pca,x_test_pca\n",
    "\n",
    "x_train,x_test=PCA_implementation(images_train,images_test)\n",
    "\n",
    "def random_forest(x_train,x_test,y_train,y_test):\n",
    "    rf=ensemble.RandomForestClassifier()\n",
    "    rf.fit(x_train,y_train)\n",
    "    y_pred=rf.predict(x_test)\n",
    "    print(\"Random Forest.................................\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    return y_pred\n",
    "\n",
    "def logistic_regression(x_train,x_test,y_train,y_test):\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    y_pred = lr.predict(x_test)\n",
    "    print(\"Logistic Regression.................................\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return y_pred\n",
    "\n",
    "def svm(x_train,x_test,y_train,y_test):\n",
    "  clf = SVC(kernel = 'linear')\n",
    "  clf.fit(x_train, y_train)\n",
    "  y_pred = clf.predict(x_test)\n",
    "  print(\"SVM.................................\")\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "  return y_pred\n",
    "\n",
    "def KNN(x_train,x_test,y_train,y_test):\n",
    "  x_axis = []\n",
    "  y_axis = []\n",
    "  for i in range(1, 26, 2):\n",
    "    clf = KNeighborsClassifier(n_neighbors = i)\n",
    "    score = cross_val_score(clf, x_train, y_train)\n",
    "    x_axis.append(i)\n",
    "    y_axis.append(score.mean())\n",
    "  max_score=(max(y_axis))\n",
    "  for i in range(len(y_axis)):\n",
    "    if y_axis[i]==max_score:\n",
    "        val=x_axis[i]\n",
    "  neigh=KNeighborsClassifier(n_neighbors = val)\n",
    "  neigh.fit(x_train, y_train)\n",
    "  y_pred = neigh.predict(x_test)\n",
    "  print(\"KNN.................................\")\n",
    "  print(classification_report(y_test, y_pred))\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "  return y_pred\n",
    "\n",
    "def Decision_Tree(x_train,x_test,y_train,y_test):\n",
    "  clf=DecisionTreeClassifier()\n",
    "  clf.fit(x_train,y_train)\n",
    "  y_pred = clf.predict(x_test)\n",
    "  print(\"Decision Tree............................\")\n",
    "  print(classification_report(y_test,y_pred))\n",
    "  print(confusion_matrix(y_test,y_pred))\n",
    "  return y_pred  \n",
    "y_train=cls_train.copy()\n",
    "y_test=cls_test.copy()\n",
    "labels=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "y_pred_1=random_forest(x_train,x_test,y_train,y_test)\n",
    "y_final_1=[]\n",
    "for i in y_pred_1:\n",
    "  y_final_1.append([labels[i]])\n",
    "with open('sub_1.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter='\\n')\n",
    "    for i in range(len(y_final_1)):\n",
    "      spamwriter.writerow(y_final_1[i])\n",
    "files.download('sub_1.csv')\n",
    "\n",
    "\n",
    "y_pred_2=logistic_regression(x_train,x_test,y_train,y_test)\n",
    "y_final_2=[]\n",
    "for i in y_pred_2:\n",
    "  y_final_2.append([labels[i]])\n",
    "with open('sub_2.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter='\\n')\n",
    "    for i in range(len(y_final_2)):\n",
    "      spamwriter.writerow(y_final_2[i])\n",
    "files.download('sub_2.csv')\n",
    "\n",
    "\n",
    "y_pred_3=KNN(x_train,x_test,y_train,y_test)\n",
    "y_final_3=[]\n",
    "for i in y_pred_3:\n",
    "  y_final_3.append([labels[i]])\n",
    "with open('sub_3.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter='\\n')\n",
    "    for i in range(len(y_final_3)):\n",
    "      spamwriter.writerow(y_final_3[i])\n",
    "files.download('sub_3.csv')\n",
    "\n",
    "y_pred_4=Decision_Tree(x_train,x_test,y_train,y_test)\n",
    "y_final_4=[]\n",
    "for i in y_pred_4:  \n",
    "  y_final_4.append([labels[i]])\n",
    "with open('sub_4.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter='\\n')\n",
    "    for i in range(len(y_final_4)):\n",
    "      spamwriter.writerow(y_final_4[i])\n",
    "files.download('sub_4.csv')\n",
    "\n",
    "\n",
    "y_pred_5=svm(x_train,x_test,y_train,y_test)\n",
    "y_final_5=[]\n",
    "for i in y_pred_5:\n",
    "  y_final_5.append([labels[i]])\n",
    "with open('sub_5.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter='\\n')\n",
    "    for i in range(len(y_final_5)):\n",
    "      spamwriter.writerow(y_final_5[i])\n",
    "files.download('sub_5.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
